{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXcge0wSzBb6"
      },
      "source": [
        "# Final Project: KidzBopifying Lyrics\n",
        "Cleo De Rocco (cleo.m.de.rocco.24@dartmouth.edu)<br>\n",
        "Abigail Kayser (abigail.e.kayser.24@dartmouth.edu)<br>\n",
        "Stefel Smith (stefel.s.smith.24@dartmouth.edu)<br>\n",
        "\n",
        "Dartmouth College, LING48, Spring 2023\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MXbg5lhzpAaA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.4 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (3.4)\n",
            "Requirement already satisfied: six in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from nltk==3.4) (1.16.0)\n",
            "Requirement already satisfied: singledispatch in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from nltk==3.4) (3.7.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/abbykayser/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pronouncing in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (0.2.0)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from pronouncing) (1.0.13)\n",
            "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (5.2.0)\n",
            "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (5.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata<6.0.0,>=5.1.0->cmudict>=0.4.0->pronouncing) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Upgrade from version in the VM\n",
        "!pip install -U nltk==3.4\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "!pip install pronouncing\n",
        "from nltk.corpus import cmudict\n",
        "import pronouncing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N1vCMVOyo8AA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import io \n",
        "import random\n",
        "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
        "from nltk.lm import MLE, NgramCounter, Vocabulary\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize, sent_tokenize, bigrams, trigrams\n",
        "from nltk.stem import SnowballStemmer\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "import numpy as np\n",
        "#import gdown\n",
        "#import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load fasstext English model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4294M  100 4294M    0     0  22.0M      0  0:03:14  0:03:14 --:--:-- 25.6M\n",
            "en.bin already exists -- do you wish to overwrite (y or n)? ^C\n"
          ]
        }
      ],
      "source": [
        "!curl -o en.bin.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "!gzip -d en.bin.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dWECRfnWp02P"
      },
      "outputs": [],
      "source": [
        "# Open file\n",
        "file = io.open('combinedLyrics.txt', encoding='utf8')\n",
        "text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 words that come after \"more\":\n",
            "['than', 'time', ',', 'I', 'dance', 'hours', 'But', 'shot', '...', \"'Cause\", '(', ')', 'things', 'they', 'now', 'And', 'gas', 'Ooh', 'So', 'It', 'exciting', 'song', 'Got', 'Sorry', 'love', 'Are', 'blue', 'you', 'Glasses', 'You', 'and', 'night', 'or', 'more', 'Then', 'Wasted', 'This', 'days', 'counting', 'Watch', 'what', 'Have', 'in', 'of', 'Oh', 'cynical', 'Atari', 'hits', 'style', '.', 'People', 'seats', 'frustrated', 'famous', 'Than', 'Said', 'Keep', 'try', 'She', 'she', 'Run', 'air', 'every', 'let', 'can', '?', 'Someone', 'like', 'smart', 'messed', 'Well', 'to']\n"
          ]
        }
      ],
      "source": [
        "# BIGRAM - processing, training, and printing sequence\n",
        "# Preprocess the tokenized text for language modelling\n",
        "n = 2\n",
        "paddedLine = [list(pad_both_ends(word_tokenize(text.lower()), n))]\n",
        "train, vocab = padded_everygram_pipeline(n, paddedLine)\n",
        "\n",
        "# Train a n-gram maximum likelihood estimation model.\n",
        "bigram_model = MLE(n) \n",
        "bigram_model.fit(train, vocab)\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "# Build frequency distribution of words that come after each word in the text\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (prev_word, next_word)\n",
        "    for prev_word, next_word in nltk.bigrams(words)\n",
        ")\n",
        "\n",
        "# Define a function to get the top 10 most likely words that come after an input word\n",
        "def get_top_words(input_word):\n",
        "    # Get the frequency distribution for the input word\n",
        "    freq_dist = cfd[input_word.lower()]\n",
        "\n",
        "    # Get the top 10 most likely words that come after the input word\n",
        "    top_10_words = freq_dist.most_common(200)\n",
        "\n",
        "    # Return the top 10 words\n",
        "    return [word[0] for word in top_10_words]\n",
        "\n",
        "# Call the function with an input word and print the top 10 most likely words that come after it.\n",
        "input_word = 'more'\n",
        "top_10_words = get_top_words(input_word)\n",
        "print(f'Top 10 words that come after \"{input_word}\":')\n",
        "print(top_10_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bad Word Bag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open file\n",
        "with io.open('bad_words.txt', encoding='utf8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the bad words\n",
        "bad_words = word_tokenize(text)\n",
        "\n",
        "# Create a bag of words from the bad words\n",
        "bag_of_words = nltk.FreqDist(bad_words)\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Function to flag bad words in a sentence\n",
        "def flag_bad_words(sentence):\n",
        "    flagged_words = []\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    for word in tokens:\n",
        "        new_word = stemmer.stem(word)\n",
        "        if word in bag_of_words:\n",
        "            flagged_words.append(word)\n",
        "        elif new_word in bag_of_words:\n",
        "            flagged_words.append(word)\n",
        "            \n",
        "    return flagged_words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phonetic Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### PHONETIC ####\n",
        "\n",
        "def calculate_phonetic_similarity(target_word, candidate_words):\n",
        "    similarity_scores = {}\n",
        "    \n",
        "    # Load the CMU Pronouncing Dictionary\n",
        "    pronouncing_dict = cmudict.dict()\n",
        "    \n",
        "    # Check if the target word exists in the CMU Pronouncing Dictionary\n",
        "    if target_word.lower() not in pronouncing_dict:\n",
        "        print(\"Target word not found in CMU Pronouncing Dictionary.\")\n",
        "        return similarity_scores\n",
        "    \n",
        "    target_phonemes = pronouncing_dict[target_word.lower()][0]\n",
        "    \n",
        "    for candidate_word in candidate_words:\n",
        "        # Check if the candidate word exists in the CMU Pronouncing Dictionary\n",
        "        if candidate_word.lower() in pronouncing_dict:\n",
        "            candidate_phonemes = pronouncing_dict[candidate_word.lower()][0]\n",
        "            # Calculate the phonetic similarity using the intersection of phonemes\n",
        "            similarity_score = len(set(target_phonemes) & set(candidate_phonemes))\n",
        "            if candidate_word in pronouncing.rhymes(target_word):\n",
        "                similarity_scores[candidate_word] = (similarity_score + 1, \"Rhymes!\")\n",
        "            else:\n",
        "                similarity_scores[candidate_word] = (similarity_score, \"\")\n",
        "        else:\n",
        "            similarity_scores[candidate_word] = (0, \"\") # Assign a similarity score of 0 if candidate word not found\n",
        "    \n",
        "    return similarity_scores\n",
        "\n",
        "def strict_rhymes(target_word, candidate_words, similarity_scores): \n",
        "    rhymes_from_ngram = []\n",
        "    rhymes = []\n",
        "    print_rhymes = []\n",
        "\n",
        "    for word in candidate_words:\n",
        "        if word in pronouncing.rhymes(target_word):\n",
        "            rhymes_from_ngram.append(word)\n",
        "\n",
        "    if len(rhymes_from_ngram) != 0:\n",
        "        print(\"Yay, there is one or more matching rhymes from the dataset! Try using:\")\n",
        "        for rhyme in rhymes_from_ngram:\n",
        "            print(rhyme, similarity_scores[rhyme])\n",
        "    else:\n",
        "        rhymes = pronouncing.rhymes(target_word)\n",
        "        print(\"No rhymes were found from the dataset. Try any of these instead!\")\n",
        "        for rhyme in rhymes:\n",
        "            if rhyme not in bad_words:\n",
        "                print_rhymes.append(rhyme)\n",
        "        print(print_rhymes)\n",
        "\n",
        "    return print_rhymes\n",
        "\n",
        "def calculate_suggestion(target_word, candidate_words):\n",
        "    similarity_scores = calculate_phonetic_similarity(target_word, candidate_words)\n",
        "    rhymes = strict_rhymes(target_word, candidate_words, similarity_scores)\n",
        "    return rhymes\n",
        "\n",
        "def phonetics(target_word, candidate_words):\n",
        "    rhymes = calculate_suggestion(target_word, candidate_words)\n",
        "    return rhymes\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantic Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "#### SEMANTICS ####\n",
        "\n",
        "embeddings = fasttext.load_model('en.bin')   #load embedding into memeory \n",
        "\n",
        "# takes in the bad word and a list of possible replacemnt words using the N gram model \n",
        "def similarity(bad_words, edits):\n",
        "    edit_scores = {}\n",
        "    for word in edits:\n",
        "        if word not in text: \n",
        "            w1 = embeddings.get_word_vector(bad_words)\n",
        "            w2 = embeddings.get_word_vector(word)\n",
        "            dist = np.linalg.norm(w2 - w1)\n",
        "            edit_scores[word] = dist \n",
        "    \n",
        "    sorted_edit_scores =  sorted(edit_scores.items(), key=lambda x:x[1])\n",
        "    return sorted_edit_scores \n",
        "\n",
        "\n",
        "def semantics(bad_word, suggested_edits):\n",
        "    print(f\"Top words Semantically Simliar to  {bad_word}.\\n\")\n",
        "    print(similarity(bad_word,suggested_edits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Transformer ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Meat of the Program, user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace(sentence, words):\n",
        "    for bad_word in words:\n",
        "        word_before = sentence[sentence.index(bad_word) - 1]\n",
        "        # print(type(sentence))\n",
        "        # print(sentence.index(bad_word))\n",
        "        # print(sentence[2], word_before)\n",
        "        candidate_words = get_top_words(word_before)\n",
        "\n",
        "        replacing = True\n",
        "        while(replacing):\n",
        "            replace_choice = input(\"Which type of replacement would you like for '\" + str(bad_word) + \"' ? Enter [p]honetic, [s]emantic, [b]oth phonetic and semantic, [t]ransformer, [a]ll, or [n]one:     \")\n",
        "            if replace_choice == \"p\":\n",
        "                replacing = False\n",
        "                print(\"You chose phonetic replacement for \" + str(bad_word) + \".\")\n",
        "                rhymes = phonetics(bad_word, candidate_words)\n",
        "            elif replace_choice == \"s\":\n",
        "                replacing = False\n",
        "                print(\"You chose semantic replacement for \" + str(bad_word) + \".\")\n",
        "                semantics(bad_word, candidate_words)\n",
        "            elif replace_choice == \"b\":\n",
        "                replacing = False\n",
        "                print(\"You chose phonetic and semantic replacement for \" + str(bad_word) + \".\")\n",
        "                both_candidates = phonetics(bad_word, [])\n",
        "                semantics(bad_word, both_candidates)\n",
        "            elif replace_choice == \"t\":\n",
        "                replacing = False\n",
        "                print(\"You chose transformer replacement for \" + str(bad_word) + \".\")\n",
        "            elif replace_choice == \"a\":\n",
        "                replacing = False\n",
        "                print(\"You chose all replacements for \" + str(bad_word) + \".\")\n",
        "            elif replace_choice == \"n\":\n",
        "                replacing = False\n",
        "                print(\"You chose not to replace \" + str(bad_word) + \".\")\n",
        "            else:\n",
        "                print(\"Please enter [p]honetic, [s]emantic, [t]ransformer, or [a]ll\")   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You entered: ' this is so fucked up'\n",
            "Flagged words: ['fucked']\n",
            "You chose phonetic and semantic replacement for fucked.\n",
            "No rhymes were found from the dataset. Try any of these instead!\n",
            "['abduct', 'bucked', 'chucked', 'conduct', 'construct', 'deconstruct', 'deduct', 'destruct', 'ducked', 'duct', 'induct', 'instruct', 'lucht', 'lucked', 'obstruct', 'plucked', 'reconstruct', 'self-destruct', 'shucked', 'sucked', 'trucked', 'tucked']\n",
            "Top words Semantically Simliar to  fucked.\n",
            "\n",
            "[('chucked', 1.0123838), ('self-destruct', 1.0574992), ('deconstruct', 1.0687076), ('reconstruct', 1.0861137), ('lucked', 1.1343501), ('construct', 1.1421357), ('plucked', 1.1502337), ('instruct', 1.1918026), ('shucked', 1.1945632), ('bucked', 1.2127217), ('destruct', 1.220667), ('obstruct', 1.2350947), ('ducked', 1.2380636), ('trucked', 1.2427006), ('conduct', 1.2672029), ('tucked', 1.2822165), ('deduct', 1.4913756), ('induct', 1.5326288), ('abduct', 1.5912011), ('lucht', 1.6631596), ('duct', 2.1647348)]\n",
            "You entered: ' put his ass to sleep'\n",
            "Flagged words: ['ass', 'sleep']\n",
            "You chose phonetic and semantic replacement for ass.\n",
            "No rhymes were found from the dataset. Try any of these instead!\n",
            "['alas', 'alsace', 'amass', 'bass', 'basse', 'blass', 'brass', 'cas', 'cass', 'chasse', 'class', 'contrasts', 'crass', 'crevasse', 'das', 'dass', 'depass', 'fahs', 'fass', 'first-class', 'forecasts', 'gas', 'gass', 'glas', 'glass', 'gras', 'grass', 'grasse', 'harass', 'hass', 'impasse', 'jas', 'jass', 'kass', 'klas', 'klass', 'kras', 'krass', 'lambastes', 'lass', 'last', 'mass', 'mass.', 'masse', 'middle-class', 'morass', 'nass', 'outlasts', 'pass', 'plas', 'plass', 'plasse', 'ras', 'repass', 'sas', 'sass', 'sasse', 'smartass', 'surpass', 'tass', 'umass', 'vanasse', 'vass', 'yass']\n",
            "Top words Semantically Simliar to  ass.\n",
            "\n",
            "[('smartass', 2.1595364), ('sasse', 2.3282194), ('blass', 2.34593), ('depass', 2.346324), ('lambastes', 2.3471892), ('crevasse', 2.3487897), ('middle-class', 2.3627837), ('crass', 2.363799), ('first-class', 2.369206), ('lass', 2.3703659), ('vanasse', 2.3820345), ('fahs', 2.382076), ('harass', 2.3836162), ('contrasts', 2.3950737), ('grasse', 2.420137), ('plasse', 2.4260018), ('brass', 2.4301085), ('krass', 2.4347143), ('hass', 2.43745), ('morass', 2.4393463), ('glass', 2.4536521), ('surpass', 2.4537318), ('forecasts', 2.462474), ('grass', 2.4646726), ('outlasts', 2.4702826), ('class', 2.4771447), ('impasse', 2.4827554), ('pass', 2.4837449), ('alas', 2.4899285), ('repass', 2.4971101), ('kras', 2.5197678), ('klass', 2.522874), ('umass', 2.5516903), ('amass', 2.571424), ('chasse', 2.6106992), ('plas', 2.6264374), ('alsace', 2.6271296), ('cass', 2.6644406), ('gras', 2.669322), ('vass', 2.6720858), ('masse', 2.6863015), ('nass', 2.7077036), ('klas', 2.7082343), ('dass', 2.7162213), ('basse', 2.7355275), ('plass', 2.7421956), ('mass', 2.761141), ('fass', 2.7627516), ('mass.', 2.7696478), ('glas', 2.7762074), ('jass', 2.818604), ('jas', 3.0483534), ('das', 3.3718529)]\n",
            "You chose not to replace sleep.\n",
            "You entered: ' this a'int a fucking game'\n",
            "Flagged words: ['fucking']\n",
            "You chose phonetic and semantic replacement for fucking.\n",
            "No rhymes were found from the dataset. Try any of these instead!\n",
            "['bucking', 'chucking', 'clucking', 'ducking', 'lucking', 'mucking', 'plucking', 'shucking', 'sucking', 'trucking', 'tucking']\n",
            "Top words Semantically Simliar to  fucking.\n",
            "\n",
            "[('chucking', 0.8093476), ('lucking', 0.8564587), ('mucking', 0.9016081), ('plucking', 0.9309954), ('ducking', 0.94889414), ('trucking', 0.9826263), ('shucking', 1.000517), ('bucking', 1.0046234), ('clucking', 1.0160036), ('tucking', 1.0252584)]\n",
            "COMPUTER: Adios!\n"
          ]
        }
      ],
      "source": [
        "responding = True\n",
        "while(responding):\n",
        "    userString = input(\"USER:     \")\n",
        "    if userString == \"exit\":\n",
        "        print(\"COMPUTER: Adios!\")\n",
        "        responding = False\n",
        "        break\n",
        "    print(\"You entered: ' \" + str(userString) + \"'\")\n",
        "    userString = userString.lower()\n",
        "    output = flag_bad_words(userString)\n",
        "    if not output:\n",
        "        print(\"No bad words found.\")\n",
        "        continue\n",
        "\n",
        "\n",
        "    replacing = True\n",
        "    while(replacing):\n",
        "        print(\"Flagged words: \" + str(output))\n",
        "        replace_want = input(\"Start replacement process for the word(s)? Enter [y] [n]:    \")\n",
        "        if replace_want == \"y\":\n",
        "            userString = userString.replace(\",\", \"\")\n",
        "            replace(userString.split(), output)\n",
        "            replacing = False\n",
        "        elif replace_want == \"n\":\n",
        "            print(\"Continue entering sentences to flag.\")\n",
        "            replacing = False\n",
        "        else:\n",
        "            print(\"Please enter [y] or [n]\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hw4-ngram-template2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
