{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXcge0wSzBb6"
      },
      "source": [
        "# Final Project: KidzBopifying Lyrics\n",
        "Cleo De Rocco (cleo.m.de.rocco.24@dartmouth.edu)<br>\n",
        "Abigail Kayser (abigail.e.kayser.24@dartmouth.edu)<br>\n",
        "Stefel Smith (stefel.s.smith.24@dartmouth.edu)<br>\n",
        "\n",
        "Dartmouth College, LING48, Spring 2023\n",
        "\n",
        "Our project is clean editing music lyrics. This is the code for the phonetic, semantic, and combinatorial (both phonetic and semantic) approaches. Call run all to initiate process. After running all, comment out the cell to load fasttext model, as this requires around 3 minutes to complete and should not be rerun once files exist. Make sure to have the files \"combinedLyrics.txt\" and \"bad_words.txt\" in the same location as the code to open them properly.\n",
        "\n",
        "We first train a bigram model on a set of clean lyrics, more specifically, the lyrics of the Kidzbop discography. We use the Laplace model because it includes smoothing, which we need for the perplexity calculation to evaluate the performance of the suggestions. We write a function using nltk's ConditionalFreqDist that will return the top 100 words following an input word.\n",
        "\n",
        "We then assemble our bag of bad words using a list of bad words, including stemming each word to make sure the bag is as robust as possible given the confines of our list.\n",
        "\n",
        "We write the phonetic and semantic functions, defined more in detail in their sections.\n",
        "\n",
        "Our final two cells are the main program, which invite the user to give input to tailor their replacements to their wishes. They input a sentence/lyric/phrase, and the program will flag potential bad words. The user can choose to start replacing or enter another input. If the user chooses to replace, they can choose between phonetic replacement, semantic replacement, phonetic --> semantic replacement (ignoring the bigram model entirely), all three at once, or not to replace the word. The replacement process continues until the user has given a reponse for all flagged bad words, at which point they will be prompted to give another sentence. To exit, the user must enter 'exit' at the first user input stage (and must follow the prompt sequence if words are flagged or if they choose to start replacement process).\n",
        "\n",
        "The phonetic replacement most often will give suggestions in a list format. In the case that the phonetic replacement finds words among the bigram model's suggested candidate words, suggestions will be given in the format: \"[word] (3, \"Rhymes!\")\", where the word is listed first, followed by its phonetic score and the word \"Rhymes!\". The phonetic score is the number of matching phonemes, with an added bonus score for rhyming. The semantic form will most often give a list of words followed by a numeric, which is the calculated distance according to the semantic function. They are sorted in decreasing order, with the first listed word being the closest semantic distance and the last being the furthest. Finally, the combinatorial approach calculates all of the rhyming words with the target word, then inputs those rhyming words to the semantic function to generate a semantically ranked list of rhyming words with the target word, in the same format as the semantic approach. \n",
        "\n",
        "Lastly, the code for calculating perplexities is included in the final cell for evaluation of the model's performance.\n",
        "\n",
        "Enjoy clean editing your lyrics!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MXbg5lhzpAaA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.4 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (3.4)\n",
            "Requirement already satisfied: six in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from nltk==3.4) (1.16.0)\n",
            "Requirement already satisfied: singledispatch in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from nltk==3.4) (3.7.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/abbykayser/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pronouncing in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (0.2.0)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from pronouncing) (1.0.13)\n",
            "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (5.2.0)\n",
            "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (5.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata<6.0.0,>=5.1.0->cmudict>=0.4.0->pronouncing) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Upgrade from version in the VM\n",
        "!pip install -U nltk==3.4\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "!pip install pronouncing\n",
        "from nltk.corpus import cmudict\n",
        "import pronouncing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "N1vCMVOyo8AA"
      },
      "outputs": [],
      "source": [
        "import io \n",
        "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
        "from nltk.lm import Laplace\n",
        "from nltk import word_tokenize, bigrams\n",
        "from nltk.stem import SnowballStemmer\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load fasstext English model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4294M  100 4294M    0     0  22.0M      0  0:03:14  0:03:14 --:--:-- 25.6M\n",
            "en.bin already exists -- do you wish to overwrite (y or n)? ^C\n"
          ]
        }
      ],
      "source": [
        "# comment out this block once run one time\n",
        "\n",
        "!curl -o en.bin.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "!gzip -d en.bin.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bigram Model\n",
        "Our bigram model is built off of a dataset of Kidzbop lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dWECRfnWp02P"
      },
      "outputs": [],
      "source": [
        "# Open file\n",
        "file = io.open('combinedLyrics.txt', encoding='utf8')\n",
        "text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 words that come after \"more\":\n",
            "['than', 'time', ',', 'I', 'dance', 'hours', 'But', 'shot', '...', \"'Cause\", '(', ')', 'things', 'they', 'now', 'And', 'gas', 'Ooh', 'So', 'It', 'exciting', 'song', 'Got', 'Sorry', 'love', 'Are', 'blue', 'you', 'Glasses', 'You', 'and', 'night', 'or', 'more', 'Then', 'Wasted', 'This', 'days', 'counting', 'Watch', 'what', 'Have', 'in', 'of', 'Oh', 'cynical', 'Atari', 'hits', 'style', '.', 'People', 'seats', 'frustrated', 'famous', 'Than', 'Said', 'Keep', 'try', 'She', 'she', 'Run', 'air', 'every', 'let', 'can', '?', 'Someone', 'like', 'smart', 'messed', 'Well', 'to']\n"
          ]
        }
      ],
      "source": [
        "# BIGRAM\n",
        "n = 2\n",
        "paddedLine = [list(pad_both_ends(word_tokenize(text.lower()), n))]\n",
        "train, vocab = padded_everygram_pipeline(n, paddedLine)\n",
        "\n",
        "# Train a n-gram Laplace model.\n",
        "bigram_model = Laplace(n) \n",
        "bigram_model.fit(train, vocab)\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Build frequency distribution of words that come after each word in the text\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (prev_word, next_word)\n",
        "    for prev_word, next_word in nltk.bigrams(words)\n",
        ")\n",
        "\n",
        "# Define a function to get the top 200 most likely words that come after an input word\n",
        "def get_top_words(input_word):\n",
        "    # Get the frequency distribution for the input word\n",
        "    freq_dist = cfd[input_word.lower()]\n",
        "\n",
        "    # Get the top 100 most likely words that come after the input word\n",
        "    top_100_words = freq_dist.most_common(100)\n",
        "\n",
        "    # Return the top 200 words\n",
        "    return [word[0] for word in top_100_words]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bad Word Bag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open file\n",
        "with io.open('bad_words.txt', encoding='utf8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the bad words\n",
        "bad_words = word_tokenize(text)\n",
        "\n",
        "# Create a set to store unique stemmed words\n",
        "bag_of_words = set()\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Add original bad words to the bag\n",
        "bag_of_words.update(bad_words)\n",
        "\n",
        "# Add stemmed versions of bad words to the bag\n",
        "for word in bad_words:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if stemmed_word not in bad_words:\n",
        "        bag_of_words.add(stemmed_word)\n",
        "\n",
        "# Function to flag bad words in a sentence\n",
        "def flag_bad_words(sentence):\n",
        "    flagged_words = []\n",
        "    for word in sentence:\n",
        "        if word in bag_of_words:\n",
        "            flagged_words.append(word)\n",
        "    return flagged_words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phonetic Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### PHONETIC ####\n",
        "\n",
        "def calculate_phonetic_similarity(target_word, candidate_words):\n",
        "    similarity_scores = {}\n",
        "    \n",
        "    # Load the CMU Pronouncing Dictionary\n",
        "    pronouncing_dict = cmudict.dict()\n",
        "    \n",
        "    # Check if the target word exists in the CMU Pronouncing Dictionary\n",
        "    if target_word.lower() not in pronouncing_dict:\n",
        "        print(\"Target word not found in CMU Pronouncing Dictionary.\")\n",
        "        return similarity_scores\n",
        "    \n",
        "    target_phonemes = pronouncing_dict[target_word.lower()][0]\n",
        "    \n",
        "    for candidate_word in candidate_words:\n",
        "        # Check if the candidate word exists in the CMU Pronouncing Dictionary\n",
        "        if candidate_word.lower() in pronouncing_dict:\n",
        "            candidate_phonemes = pronouncing_dict[candidate_word.lower()][0]\n",
        "            # Calculate the phonetic similarity using the intersection of phonemes\n",
        "            similarity_score = len(set(target_phonemes) & set(candidate_phonemes))\n",
        "            if candidate_word in pronouncing.rhymes(target_word):\n",
        "                similarity_scores[candidate_word] = (similarity_score + 1, \"Rhymes!\")\n",
        "            else:\n",
        "                similarity_scores[candidate_word] = (similarity_score, \"\")\n",
        "        else:\n",
        "            similarity_scores[candidate_word] = (0, \"\") # Assign a similarity score of 0 if candidate word not found\n",
        "    \n",
        "    return similarity_scores\n",
        "\n",
        "# To find all of the words that strictly rhyme with the target word\n",
        "def strict_rhymes(target_word, candidate_words, similarity_scores): \n",
        "    rhymes_from_ngram = []\n",
        "    rhymes = []\n",
        "    print_rhymes = []\n",
        "\n",
        "    # append all words in candidates that rhyme to a list\n",
        "    for word in candidate_words:\n",
        "        if word in pronouncing.rhymes(target_word):\n",
        "            rhymes_from_ngram.append(word)\n",
        "\n",
        "    # print out the rhymes from the bigram and their simialrity scores calculated in calculate_phonetic_similarity\n",
        "    if len(rhymes_from_ngram) != 0:\n",
        "        print(\"Yay, there is one or more matching rhymes from the dataset! Try using:\")\n",
        "        for rhyme in rhymes_from_ngram:\n",
        "            print(rhyme, similarity_scores[rhyme])\n",
        "    # otherwise print a (cleaned) list of all of the words that rhyme with the target word\n",
        "    else:\n",
        "        rhymes = pronouncing.rhymes(target_word)\n",
        "        print(\"No rhymes were found from the dataset. Try any of these instead!\")\n",
        "        for rhyme in rhymes:\n",
        "            if rhyme not in bad_words:\n",
        "                print_rhymes.append(rhyme)\n",
        "        print(print_rhymes)\n",
        "\n",
        "    return print_rhymes\n",
        "\n",
        "def calculate_suggestion(target_word, candidate_words):\n",
        "    similarity_scores = calculate_phonetic_similarity(target_word, candidate_words)\n",
        "    rhymes = strict_rhymes(target_word, candidate_words, similarity_scores)\n",
        "    return rhymes\n",
        "\n",
        "def phonetics(target_word, candidate_words):\n",
        "    rhymes = calculate_suggestion(target_word, candidate_words)\n",
        "    return rhymes\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantic Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "#### SEMANTICS ####\n",
        "\n",
        "embeddings = fasttext.load_model('en.bin')   #load embedding into memeory \n",
        "\n",
        "# takes in the bad word and a list of possible replacemnt words using the bigram model and calculates euclidean distance\n",
        "def similarity(bad_words, edits):\n",
        "    edit_scores = {}\n",
        "    for word in edits:\n",
        "        if word not in text: \n",
        "            w1 = embeddings.get_word_vector(bad_words)\n",
        "            w2 = embeddings.get_word_vector(word)\n",
        "            dist = np.linalg.norm(w2 - w1)\n",
        "            edit_scores[word] = dist \n",
        "    \n",
        "    sorted_edit_scores =  sorted(edit_scores.items(), key=lambda x:x[1])\n",
        "    return sorted_edit_scores \n",
        "\n",
        "\n",
        "def semantics(bad_word, suggested_edits):\n",
        "    print(f\"Top words Semantically Simliar to  {bad_word}.\\n\")\n",
        "    print(similarity(bad_word,suggested_edits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replacement Program, user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this cell contains helper functions\n",
        "# prompts user for input and calls replacement functions\n",
        "def replace(sentence, words):\n",
        "    for bad_word in words:\n",
        "        \n",
        "        # get the word before and the candidate words\n",
        "        word_before = sentence[sentence.index(bad_word) - 1]\n",
        "        candidate_words = get_top_words(word_before)\n",
        "\n",
        "        replacing = True\n",
        "        while(replacing):\n",
        "            replace_choice = input(\"Which type of replacement would you like for '\" + str(bad_word) + \"' ? Enter [p]honetic, [s]emantic, [b]oth phonetic and semantic, [a]ll, or [n]one:     \")\n",
        "            \n",
        "            # PHONETIC\n",
        "            if replace_choice == \"p\":  \n",
        "                replacing = False\n",
        "                print(\"You chose phonetic replacement for \" + str(bad_word) + \".\")\n",
        "                rhymes = phonetics(bad_word, candidate_words)\n",
        "\n",
        "            # SEMANTIC\n",
        "            elif replace_choice == \"s\":\n",
        "                replacing = False\n",
        "                print(\"You chose semantic replacement for \" + str(bad_word) + \".\")\n",
        "                semantics(bad_word, candidate_words)\n",
        "            \n",
        "            # COMBINATORIAL\n",
        "            elif replace_choice == \"b\":\n",
        "                replacing = False\n",
        "                print(\"You chose phonetic and semantic replacement for \" + str(bad_word) + \".\")\n",
        "                both_candidates = phonetics(bad_word, [])\n",
        "                semantics(bad_word, both_candidates)\n",
        "            \n",
        "            # ALL \n",
        "            elif replace_choice == \"a\":\n",
        "                replacing = False\n",
        "                print(\"You chose all replacements for \" + str(bad_word) + \".\")\n",
        "                rhymes = phonetics(bad_word, candidate_words)\n",
        "                semantics(bad_word, candidate_words)\n",
        "                both_candidates = phonetics(bad_word, [])\n",
        "                semantics(bad_word, both_candidates)\n",
        "            \n",
        "            # If the user chooses not to replace the word\n",
        "            elif replace_choice == \"n\":\n",
        "                replacing = False\n",
        "                print(\"You chose not to replace \" + str(bad_word) + \".\")\n",
        "            \n",
        "            else:\n",
        "                print(\"Please enter [p]honetic, [s]emantic, [b]oth phonetic and semantic, [a]ll, or [n]one\")  \n",
        "\n",
        "# Helper function to remove punctuation\n",
        "def clean(userString): \n",
        "\n",
        "    userString = userString.replace(\",\", \"\")\n",
        "    userString = userString.replace(\"' \", \"  \")\n",
        "    userString = userString.replace(\".\", \"\")\n",
        "    userString = userString.replace(\"!\", \"\")\n",
        "    userString = userString.replace(\"?\", \"\")\n",
        "    userString = userString.replace(\" '\", \"  \")\n",
        "    userString = userString.lower()\n",
        "\n",
        "\n",
        "    return userString\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Running this cell initiates the code\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "responding = True\n",
        "while(responding):\n",
        "    userString = input(\"USER:     \")\n",
        "    \n",
        "    # exit if the user is done\n",
        "    if userString == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        responding = False\n",
        "        break\n",
        "    \n",
        "    # print and clean the string\n",
        "    print(\"You entered: '\" + str(userString) + \"'\")\n",
        "    userString = clean(userString)\n",
        "    \n",
        "    # split the string and stem it to find stemmed bad words\n",
        "    input_string = userString.split()\n",
        "    stemmed_string = []\n",
        "    output = flag_bad_words(input_string)       # flag initial bad words\n",
        "    \n",
        "    for word in input_string:\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        if word not in output:                      # do not add repeats\n",
        "            stemmed_string.append(stemmed_word)\n",
        "        else:\n",
        "            stemmed_string.append(\"\")\n",
        "        \n",
        "    # add stem bad words to bad words\n",
        "    stemmed_output = flag_bad_words(stemmed_string)\n",
        "    for word in stemmed_output:\n",
        "        output.append(input_string[stemmed_string.index(word)])\n",
        "\n",
        "\n",
        "    if not output:\n",
        "        print(\"No bad words found.\")\n",
        "        continue\n",
        "\n",
        "    # initiate replacement process if wished by the user\n",
        "    replacing = True\n",
        "    while(replacing):\n",
        "        print(\"Flagged words: \" + str(output))\n",
        "        replace_want = input(\"Start replacement process for the word(s)? Enter [y] [n]:    \")\n",
        "        \n",
        "        if replace_want == \"y\":\n",
        "            replace(input_string, output)\n",
        "            replacing = False\n",
        "        \n",
        "        elif replace_want == \"n\":\n",
        "            print(\"Continue entering sentences to flag.\")\n",
        "            replacing = False\n",
        "        \n",
        "        else:\n",
        "            print(\"Please enter [y] or [n]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PP([enter a sentence here to calculate the perplexity!]):6458.06842686986\n"
          ]
        }
      ],
      "source": [
        "test_sentences = [\"[enter a sentence here to calculate the perplexity!]\"]\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in test_sentences]\n",
        "\n",
        "test_data = [bigrams(t,  pad_right=False, pad_left=False) for t in tokenized_text]\n",
        "for i, test in enumerate(test_data):\n",
        "  print(\"PP({0}):{1}\".format(test_sentences[i], bigram_model.perplexity(test)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hw4-ngram-template2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
