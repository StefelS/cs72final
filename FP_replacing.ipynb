{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXcge0wSzBb6"
      },
      "source": [
        "# Final Project: KidzBopifying Lyrics\n",
        "Cleo De Rocco (cleo.m.de.rocco.24@dartmouth.edu)<br>\n",
        "Abigail Kayser (abigail.e.kayser.24@dartmouth.edu)<br>\n",
        "Stefel Smith (stefel.s.smith.24@dartmouth.edu)<br>\n",
        "\n",
        "Dartmouth College, LING48, Spring 2023\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MXbg5lhzpAaA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.4 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (3.4)\n",
            "Requirement already satisfied: singledispatch in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from nltk==3.4) (3.7.0)\n",
            "Requirement already satisfied: six in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from nltk==3.4) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/abbykayser/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pronouncing in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (0.2.0)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from pronouncing) (1.0.13)\n",
            "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (5.2.0)\n",
            "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (5.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/abbykayser/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata<6.0.0,>=5.1.0->cmudict>=0.4.0->pronouncing) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Upgrade from version in the VM\n",
        "!pip install -U nltk==3.4\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "!pip install pronouncing\n",
        "from nltk.corpus import cmudict\n",
        "import pronouncing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N1vCMVOyo8AA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import io \n",
        "import random\n",
        "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
        "from nltk.lm import MLE, NgramCounter, Vocabulary\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize, sent_tokenize, bigrams, trigrams\n",
        "from nltk.stem import SnowballStemmer\n",
        "#import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dWECRfnWp02P"
      },
      "outputs": [],
      "source": [
        "# Open file\n",
        "file = io.open('combinedLyrics.txt', encoding='utf8')\n",
        "text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BIGRAM - processing, training, and printing sequence\n",
        "# Preprocess the tokenized text for language modelling\n",
        "n = 2\n",
        "paddedLine = [list(pad_both_ends(word_tokenize(text.lower()), n))]\n",
        "train, vocab = padded_everygram_pipeline(n, paddedLine)\n",
        "\n",
        "# Train a n-gram maximum likelihood estimation model.\n",
        "bigram_model = MLE(n) \n",
        "bigram_model.fit(train, vocab)\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "# Build frequency distribution of words that come after each word in the text\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (prev_word, next_word)\n",
        "    for prev_word, next_word in nltk.bigrams(words)\n",
        ")\n",
        "\n",
        "# Define a function to get the top 10 most likely words that come after an input word\n",
        "def get_top_words(input_word):\n",
        "    # Get the frequency distribution for the input word\n",
        "    freq_dist = cfd[input_word.lower()]\n",
        "\n",
        "    # Get the top 10 most likely words that come after the input word\n",
        "    top_10_words = freq_dist.most_common(100)\n",
        "\n",
        "    # Return the top 10 words\n",
        "    return [word[0] for word in top_10_words]\n",
        "\n",
        "# # Call the function with an input word and print the top 10 most likely words that come after it.\n",
        "# input_word = 'more'\n",
        "# top_10_words = get_top_10_words(input_word)\n",
        "# print(f'Top 10 words that come after \"{input_word}\":')\n",
        "# print(top_10_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bad Word Bag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open file\n",
        "with io.open('bad_words.txt', encoding='utf8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the bad words\n",
        "bad_words = word_tokenize(text)\n",
        "\n",
        "# Create a bag of words from the bad words\n",
        "bag_of_words = nltk.FreqDist(bad_words)\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Function to flag bad words in a sentence\n",
        "def flag_bad_words(sentence):\n",
        "    flagged_words = []\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    for word in tokens:\n",
        "        new_word = stemmer.stem(word)\n",
        "        if word in bag_of_words:\n",
        "            flagged_words.append(word)\n",
        "        elif new_word in bag_of_words:\n",
        "            flagged_words.append(word)\n",
        "            \n",
        "    return flagged_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phonetic Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### PHONETIC ####\n",
        "\n",
        "def calculate_phonetic_similarity(target_word, candidate_words):\n",
        "    similarity_scores = []\n",
        "    \n",
        "    # Load the CMU Pronouncing Dictionary\n",
        "    pronouncing_dict = cmudict.dict()\n",
        "    \n",
        "    # Check if the target word exists in the CMU Pronouncing Dictionary\n",
        "    if target_word.lower() not in pronouncing_dict:\n",
        "        print(\"Target word not found in CMU Pronouncing Dictionary.\")\n",
        "        return similarity_scores\n",
        "    \n",
        "    target_phonemes = pronouncing_dict[target_word.lower()][0]\n",
        "    \n",
        "    for candidate_word in candidate_words:\n",
        "        # Check if the candidate word exists in the CMU Pronouncing Dictionary\n",
        "        if candidate_word.lower() in pronouncing_dict:\n",
        "            candidate_phonemes = pronouncing_dict[candidate_word.lower()][0]\n",
        "            # Calculate the phonetic similarity using the intersection of phonemes\n",
        "            similarity_score = len(set(target_phonemes) & set(candidate_phonemes))\n",
        "            if candidate_word in pronouncing.rhymes(target_word):\n",
        "                similarity_scores.append((candidate_word, similarity_score + 1, \"Rhymes!\"))\n",
        "            else:\n",
        "                similarity_scores.append((candidate_word, similarity_score, \"\"))\n",
        "        else:\n",
        "            similarity_scores.append((candidate_word, 0, \"\"))  # Assign a similarity score of 0 if candidate word not found\n",
        "    \n",
        "    return similarity_scores\n",
        "\n",
        "def strict_rhymes(target_word, candidate_words): \n",
        "    rhymes_from_ngram = []\n",
        "    rhymes = []\n",
        "\n",
        "    yes = \"wheeze\" in pronouncing.rhymes(\"cheese\")\n",
        "    for word in candidate_words:\n",
        "        if word in pronouncing.rhymes(target_word):\n",
        "            rhymes_from_ngram.append(word)\n",
        "\n",
        "    if len(rhymes_from_ngram) != 0:\n",
        "        print(\"Yay, there is one or more matching rhymes from the dataset! Try using:\")\n",
        "        print(rhymes_from_ngram)\n",
        "    else:\n",
        "        rhymes = pronouncing.rhymes(target_word)\n",
        "        print(\"No rhymes were found from the dataset. Try any of these instead!\")\n",
        "        print(rhymes)\n",
        "    return\n",
        "\n",
        "def calculate_suggestion(target_word, candidate_words):\n",
        "    similarity_scores = calculate_phonetic_similarity(target_word, candidate_words)\n",
        "    \n",
        "    i = 0\n",
        "    for candidate, score, note in similarity_scores:\n",
        "        if (note == \"Rhymes!\"):\n",
        "            print(candidate, note)\n",
        "        if note == \"\":\n",
        "            i += 1\n",
        "        if i == len(similarity_scores):\n",
        "            strict_rhymes(target_word, candidate_words)\n",
        "\n",
        "def phonetics(target_word, candidate_words):\n",
        "    calculate_suggestion(target_word, candidate_words)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantic Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### SEMANTICS ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Transformer ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Meat of the Program, user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace(sentence, words):\n",
        "    for bad_word in words:\n",
        "        word_before = sentence[sentence.index(bad_word) - 1]\n",
        "        # print(type(sentence))\n",
        "        # print(sentence.index(bad_word))\n",
        "        # print(sentence[2], word_before)\n",
        "        candidate_words = get_top_words(word_before)\n",
        "        replacing = True\n",
        "        while(replacing):\n",
        "            replace_choice = input(\"Which type of replacement would you like for '\" + str(bad_word) + \"' ? Enter [p]honetic, [s]emantic, [t]ransformer, or [a]ll:     \")\n",
        "            if replace_choice == \"p\":\n",
        "                replacing = False\n",
        "                print(\"You chose phonetic replacement for \" + str(bad_word) + \".\")\n",
        "                phonetics(bad_word, candidate_words)\n",
        "            elif replace_choice == \"s\":\n",
        "                replacing = False\n",
        "                print(\"You chose semantic replacement for \" + str(bad_word) + \".\")\n",
        "            elif replace_choice == \"t\":\n",
        "                replacing = False\n",
        "                print(\"You chose transformer replacement for \" + str(bad_word) + \".\")\n",
        "            elif replace_choice == \"a\":\n",
        "                replacing = False\n",
        "                print(\"You chose all replacements for \" + str(bad_word) + \".\")\n",
        "            else:\n",
        "                print(\"Please enter [p]honetic, [s]emantic, [t]ransformer, or [a]ll\")   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You entered: ' this is shit'\n",
            "Flagged words: ['shit']\n",
            "this is shit\n",
            "You chose phonetic replacement for shit.\n",
            "it 3 Rhymes!\n",
            "sit 3 Rhymes!\n",
            "COMPUTER: Adios!\n"
          ]
        }
      ],
      "source": [
        "responding = True\n",
        "while(responding):\n",
        "    userString = input(\"USER:     \")\n",
        "    if userString == \"exit\":\n",
        "        print(\"COMPUTER: Adios!\")\n",
        "        responding = False\n",
        "        break\n",
        "    print(\"You entered: ' \" + str(userString) + \"'\")\n",
        "    output = flag_bad_words(userString)\n",
        "    if not output:\n",
        "        print(\"No bad words found.\")\n",
        "        continue\n",
        "\n",
        "\n",
        "    replacing = True\n",
        "    while(replacing):\n",
        "        print(\"Flagged words: \" + str(output))\n",
        "        replace_want = input(\"Start replacement process? Enter [y] [n]:    \")\n",
        "        if replace_want == \"y\":\n",
        "            print(userString)\n",
        "            replace(userString.split(), output)\n",
        "            replacing = False\n",
        "        elif replace_want == \"n\":\n",
        "            print(\"Continue entering sentences to flag.\")\n",
        "            replacing = False\n",
        "        else:\n",
        "            print(\"Please enter [y] or [n]\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hw4-ngram-template2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
