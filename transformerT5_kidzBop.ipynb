{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaJOuIOH0vKG"
      },
      "source": [
        "Applying T5 to Cleaning Explicit Lyrics\n",
        "Cleo De Rocco, Stefel Smith, Abigail Kayser: LING48, Spring 2023<br>\n",
        "\n",
        "This code has been modified from:<br>\n",
        "https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb<br>\n",
        "https://huggingface.co/transformers/master/community.html#community-notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-qJCOwQeyaF"
      },
      "source": [
        "# Fine Tuning Transformer for Cleaning Explicit Lyrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AO3xgj4eyaI"
      },
      "source": [
        "\n",
        "### Introduction\n",
        "\n",
        "In this tutorial we will be fine tuning a transformer model for **Cleaning Explicit Lyrics Task**. \n",
        "\n",
        "#### Flow of the notebook\n",
        "\n",
        "The notebook will be divided into separate sections to provide a organized walk through for the process used. This process can be modified for individual use cases. The sections are:\n",
        "\n",
        "1. [Preparing Environment and Importing Libraries](#section01)\n",
        "2. [Preparing the Dataset for data processing: Class](#section02)\n",
        "3. [Fine Tuning the Model: Function](#section03)\n",
        "4. [Validating the Model Performance: Function](#section04)\n",
        "5. [Main Function](#section05)\n",
        "    * [Importing and Pre-Processing the domain data](#section502)\n",
        "    * [Creation of Dataset and Dataloader](#section503)\n",
        "    * [Neural Network and Optimizer](#section504)\n",
        "    * [Training Model](#section505)\n",
        "    * [Validation and generation of Summary](#section506)\n",
        "6. [Examples of the Summary Generated from the model](#section06)\n",
        "\n",
        "\n",
        "#### Technical Details\n",
        "\n",
        "This script leverages on multiple tools designed by other teams. Details of the tools used below. Please ensure that these elements are present in your setup to successfully implement this script.\n",
        "\n",
        "- **Data**:\n",
        "\t- We are using the -------\n",
        "\t- This dataset is the collection created from ------ details that are listed below. \n",
        "\t- There are`----` rows of data.  Where each row has the following data-point:\n",
        "\t\t- **author** : Author of the article\n",
        "\t\t- **text**: This is the summary of the article\n",
        "\t\t- **ctext**: This is the complete article\n",
        "\n",
        "\n",
        "- **Language Model Used**: \n",
        "    - This notebook uses one of the most recent and novel transformers model ***T5***. [Research Paper](https://arxiv.org/abs/1910.10683)    \n",
        "    - ***T5*** in many ways is one of its kind transformers architecture that not only gives state of the art results in many NLP tasks, but also has a very radical approach to NLP tasks.\n",
        "    - **Text-2-Text** - According to the graphic taken from the T5 paper. All NLP tasks are converted to a **text-to-text** problem. Tasks such as translation, classification, summarization and question answering, all of them are treated as a text-to-text conversion problem, rather than seen as separate unique problem statements.\n",
        "    - **Unified approach for NLP Deep Learning** - Since the task is reflected purely in the text input and output, you can use the same model, objective, training procedure, and decoding process to ANY task. Above framework can be used for any task - show Q&A, summarization, etc. \n",
        "   - We will be taking inputs from the T5 paper to prepare our dataset prior to fine tuning and training.    \n",
        "   - [Documentation for python](https://huggingface.co/transformers/model_doc/t5.html)\n",
        "\n",
        "![**Each NLP problem as a “text-to-text” problem** - input: text, output: text](https://miro.medium.com/max/4006/1*D0J1gNQf8vrrUpKeyD8wPA.png) \n",
        "\n",
        "\n",
        "- Hardware Requirements: \n",
        "\t- Python 3.6 and above\n",
        "\t- Pytorch, Transformers and\n",
        "\t- All the stock Python ML Library\n",
        "\t- GPU enabled setup \n",
        "   \n",
        "\n",
        "- **Script Objective**:\n",
        "\t- The objective of this script is to fine tune ***T5 *** to be able to clean text, that a close to or better than the actual cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81KEQpTeyaN"
      },
      "source": [
        "<a id='section01'></a>\n",
        "### Preparing Environment and Importing Libraries\n",
        "\n",
        "At this step we will be installing the necessary libraries followed by importing the libraries and modules needed to run our script. \n",
        "We will be installing:\n",
        "* transformers\n",
        "\n",
        "Libraries imported are:\n",
        "* Pandas\n",
        "* Pytorch\n",
        "* Pytorch Utils for Dataset and Dataloader\n",
        "* Transformers\n",
        "* T5 Model and Tokenizer\n",
        "\n",
        "Followed by that we will prepare the device for CUDA execeution. This configuration is needed if you want to leverage on onboard GPU. First we will check the GPU avaiable to us, using the nvidia command followed by defining our device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WD_vnyLXZQzD",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4328151b-3123-480b-b26e-b72ef7adb502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers -q\n",
        "!pip install sentencepiece\n",
        "\n",
        "# Code for TPU packages install\n",
        "# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEpnQDnWQ3ny",
        "outputId": "bac71a7e-26a6-45b8-f57c-ceb6aaf208f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VA90T7-GorZy"
      },
      "outputs": [],
      "source": [
        "# Importing stock libraries\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o33gUiikQ3ny"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K27qL5VTQ3ny"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nW3bP0TGQ3nz"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "odLYkZHWQ3nz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uySYN08YQ3nz"
      },
      "outputs": [],
      "source": [
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Gyz3z37iQ3nz"
      },
      "outputs": [],
      "source": [
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_fI2GHkjm-S8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "a156a259-e8ff-4161-cebd-de9ddcb7b690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16nJwJ85fxOk_7RvT2zaOdUyu8bCqG5yb\n",
            "To: /content/news_summary.csv\n",
            "100%|██████████| 107k/107k [00:00<00:00, 90.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'news_summary.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Download sample data\n",
        "url = \"https://drive.google.com/uc?id=16nJwJ85fxOk_7RvT2zaOdUyu8bCqG5yb\"\n",
        "# \"https://drive.google.com/uc?id=1XYS3qle4_h2h8Ggsk2da4BAu28MKbdP1\"\n",
        "#\"https://drive.google.com/uc?id=18Y7ubgjhaJhuVAgArMYXski0yDxYlbH-\"\n",
        "#\"https://drive.google.com/uc?id=1KV_Ft2Cd0iBZuwtDdElXTJm8JoiOIBau\" \n",
        "#\"https://drive.google.com/uc?id=1LE8bcELLTwtUHpxYyuVP7CsRBCNi4DwV\"\n",
        "output = 'news_summary.csv'\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KvPxXdKJguYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca13d0e2-1a12-449a-8a8e-5b98e80a7737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun  5 05:36:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NLxxwd1scQNv"
      },
      "outputs": [],
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Preparing for TPU usage\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyQFvFhUeyaT"
      },
      "source": [
        "<a id='section02'></a>\n",
        "### Preparing the Dataset for data processing: Class\n",
        "\n",
        "We will start with creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. This dataset will be used the the Dataloader method that will feed  the data in batches to the neural network for suitable training and processing. \n",
        "The Dataloader and Dataset will be used inside the `main()`.\n",
        "Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "#### *CustomDataset* Dataset Class\n",
        "- This class is defined to accept the Dataframe as input and generate tokenized output that is used by the **T5** model for training. \n",
        "- We are using the **T5** tokenizer to tokenize the data in the `text` and `ctext` column of the dataframe. \n",
        "- The tokenizer uses the ` batch_encode_plus` method to perform tokenization and generate the necessary outputs, namely: `source_id`, `source_mask` from the actual text and `target_id` and `target_mask` from the summary text.\n",
        "- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer)\n",
        "- The *CustomDataset* class is used to create 2 datasets, for training and for validation.\n",
        "- *Training Dataset* is used to fine tune the model: **80% of the original data**\n",
        "- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training. \n",
        "\n",
        "#### Dataloader: Called inside the `main()`\n",
        "- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of data loaded to the memory and then passed to the neural network needs to be controlled.\n",
        "- This control is achieved using the parameters such as `batch_size` and `max_len`.\n",
        "- Training and Validation dataloaders are used in the training and validation part of the flow respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "932p8NhxeNw4"
      },
      "outputs": [],
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.kbLyric # text is summarized text -> kidz bop\n",
        "        self.ctext = self.data.ogLyric # ctext is complete text -> explicit\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4dYezzDeyaU"
      },
      "source": [
        "<a id='section03'></a>\n",
        "### Fine Tuning the Model: Function\n",
        "\n",
        "Here we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network. \n",
        "\n",
        "This function is called in the `main()`\n",
        "\n",
        "Following events happen in this function to fine tune the neural network:\n",
        "- The epoch, tokenizer, model, device details, testing_ dataloader and optimizer are passed to the `train ()` when its called from the `main()`\n",
        "- The dataloader passes data to the model based on the batch size.\n",
        "- `language_model_labels` are calculated from the `target_ids` also, `source_id` and `attention_mask` are extracted.\n",
        "- The model outputs first element gives the loss for the forward pass. \n",
        "- Loss value is used to optimize the weights of the neurons in the network.\n",
        "- After every 500 steps the loss value is printed in the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SaPAR7TWmxoM"
      },
      "outputs": [],
      "source": [
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXGRrBZOeyaU"
      },
      "source": [
        "<a id='section04'></a>\n",
        "### Validating the Model Performance: Function\n",
        "\n",
        "During the validation stage we pass the unseen data(Testing Dataset), trained model, tokenizer and device details to the function to perform the validation run. This step generates new summary for dataset that it has not seen during the training session. \n",
        "\n",
        "This function is called in the `main()`\n",
        "\n",
        "This unseen data is the 20% of `news_summary.csv` which was seperated during the Dataset creation stage. \n",
        "During the validation stage the weights of the model are not updated. We use the generate method for generating new text for the summary. \n",
        "\n",
        "It depends on the `Beam-Search coding` method developed for sequence generation for models with LM head. \n",
        "\n",
        "The generated text and originally summary are decoded from tokens to text and returned to the `main()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j9TNdHlQ0CLz"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    explicits = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "            #print(data)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True,\n",
        "                temperature = 0.5\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            #explicit = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in ids]\n",
        "\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "            \n",
        "            # explicits.append(explicit)\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals #, explicits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCu9u2R6eyaV"
      },
      "source": [
        "<a id='section05'></a>\n",
        "### Main Function\n",
        "\n",
        "The `main()` as the name suggests is the central location to execute all the functions/flows created above in the notebook. The following steps are executed in the `main()`:\n",
        "\n",
        "<a id='section502'></a>\n",
        "#### Importing and Pre-Processing the domain data\n",
        "\n",
        "We will be working with the data and preparing it for fine tuning purposes. \n",
        "*Assuming that the `news_summary.csv` is already downloaded in your `data` folder*\n",
        "\n",
        "* The file is imported as a dataframe and give it the headers as per the documentation.\n",
        "* Cleaning the file to remove the unwanted columns.\n",
        "* A new string is added to the main article column `summarize: ` prior to the actual article. This is done because **T5** had similar formatting for the summarization dataset. \n",
        "* The final Dataframe will be something like this:\n",
        "\n",
        "|text|ctext|\n",
        "|--|--|\n",
        "|summary-1|summarize: article 1|\n",
        "|summary-2|summarize: article 2|\n",
        "|summary-3|summarize: article 3|\n",
        "\n",
        "* Top 5 rows of the dataframe are printed on the console.\n",
        "\n",
        "<a id='section503'></a>\n",
        "#### Creation of Dataset and Dataloader\n",
        "\n",
        "* The updated dataframe is divided into 80-20 ratio for test and validation. \n",
        "* Both the data-frames are passed to the `CustomerDataset` class for tokenization of the new articles and their summaries.\n",
        "* The tokenization is done using the length parameters passed to the class.\n",
        "* Train and Validation parameters are defined and passed to the `pytorch Dataloader contstruct` to create `train` and `validation` data loaders.\n",
        "* These dataloaders will be passed to `train()` and `validate()` respectively for training and validation action.\n",
        "* The shape of datasets is printed in the console.\n",
        "\n",
        "\n",
        "<a id='section504'></a>\n",
        "#### Neural Network and Optimizer\n",
        "\n",
        "* In this stage we define the model and optimizer that will be used for training and to update the weights of the network. \n",
        "* We are using the `t5-base` transformer model for our project. You can read about the `T5 model` and its features above. \n",
        "* We use the `T5ForConditionalGeneration.from_pretrained(\"t5-base\")` commad to define our model. The `T5ForConditionalGeneration` adds a Language Model head to our `T5 model`. The Language Model head allows us to generate text based on the training of `T5 model`.\n",
        "* We are using the `Adam` optimizer for our project. This has been a standard for all our tutorials and is something that can be changed updated to see how different optimizer perform with different learning rates. \n",
        "* There is also a scope for doing more with Optimizer such a decay, momentum to dynamically update the Learning rate and other parameters. All those concepts have been kept out of scope for these tutorials. \n",
        "\n",
        "\n",
        "<a id='section505'></a>\n",
        "#### Training Model\n",
        "\n",
        "* Loss at every 500th step is printed on the console.\n",
        "\n",
        "<a id='section506'></a>\n",
        "#### Validation and generation of Summary\n",
        "\n",
        "* After the training is completed, the validation step is initiated.\n",
        "* As defined in the validation function, the model weights are not updated. We use the fine tuned model to generate new summaries based on the article text.\n",
        "* An output is printed on the console giving a count of how many steps are complete after every 100th step. \n",
        "* The original summary and generated summary are converted into a list and returned to the main function. \n",
        "* Both the lists are used to create the final dataframe with 2 columns **Generated Summary** and **Actual Summary**\n",
        "* The dataframe is saved as a csv file in the local drive.\n",
        "* A qualitative analysis can be done with the Dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZtNs9ytpCow2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0bb19d-c745-4429-d403-bc77502f9c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             kbLyric  \\\n",
            "0  Bought matching diamonds so they know what thi...   \n",
            "1              Baby, come on wave your hands tonight   \n",
            "2        Set it off with a track dancing in the dark   \n",
            "3                    I get by, and I love to lay low   \n",
            "4     Well, I knew me a boy, and yes he was a friend   \n",
            "\n",
            "                                             ogLyric  \n",
            "0  Clean: Bought matching diamonds for six of my ...  \n",
            "1         Clean: Baby, let me blow your mind tonight  \n",
            "2  Clean: Get you off with the touch, dancing in ...  \n",
            "3           Clean: I get high, and I love to get low  \n",
            "4  Clean: Well, I had me a boy, turned him into a...  \n",
            "FULL Dataset: (815, 2)\n",
            "TRAIN Dataset: (652, 2)\n",
            "TEST Dataset: (163, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss:  8.247200012207031\n",
            "Epoch: 1, Loss:  1.4894627332687378\n",
            "Epoch: 2, Loss:  1.0374895334243774\n",
            "Epoch: 3, Loss:  0.5651116371154785\n",
            "Epoch: 4, Loss:  0.3908073306083679\n"
          ]
        }
      ],
      "source": [
        "# This should take about 30 minutes (29 when I tried it)\n",
        "\n",
        "configTRAIN_BATCH_SIZE = 2 #2\n",
        "configVALID_BATCH_SIZE = 2 #2\n",
        "configSeed = 42\n",
        "configMAX_LEN = 150\n",
        "configLYRIC_LEN = 100\n",
        "configLEARNING_RATE = 1e-4\n",
        "configTRAIN_EPOCHS = 5\n",
        "configVAL_EPOCHS = 3\n",
        "\n",
        "# Set random seeds and deterministic pytorch for reproducibility\n",
        "torch.manual_seed(configSeed) # pytorch random seed\n",
        "np.random.seed(configSeed) # numpy random seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# tokenzier for encoding the text\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Importing and Pre-Processing the domain data\n",
        "# Selecting the needed columns only. \n",
        "# Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
        "df = pd.read_csv('news_summary.csv',encoding='latin-1')\n",
        "df = df[['kbLyric','ogLyric']]\n",
        "df.ogLyric = 'Clean: ' + df.ogLyric\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Creation of Dataset and Dataloader\n",
        "# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state = configSeed)\n",
        "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "\n",
        "# Creating the Training and Validation dataset for further creation of Dataloader\n",
        "training_set = CustomDataset(train_dataset, tokenizer, configMAX_LEN, configLYRIC_LEN)\n",
        "val_set = CustomDataset(val_dataset, tokenizer, configMAX_LEN, configLYRIC_LEN)\n",
        "\n",
        "# Defining the parameters for creation of dataloaders\n",
        "train_params = {\n",
        "    'batch_size': configTRAIN_BATCH_SIZE,\n",
        "    'shuffle': True,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "val_params = {\n",
        "    'batch_size': configVALID_BATCH_SIZE,\n",
        "    'shuffle': False,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "\n",
        "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=configLEARNING_RATE)\n",
        "\n",
        "for epoch in range(configTRAIN_EPOCHS):\n",
        "    train(epoch, tokenizer, model, device, training_loader, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "# Saving the dataframe as predictions.csv\n",
        "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "for epoch in range(configVAL_EPOCHS):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    filename_csv = \"predictions_e\" + str(configTRAIN_EPOCHS) + \".csv\"\n",
        "    final_df.to_csv(filename_csv)\n",
        "    print('Output Files generated for review')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdd-rgMdfp3N",
        "outputId": "36f6665b-1aed-43a3-892c-d1508c845bf9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
            "Completed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Files generated for review\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 0\n",
            "Output Files generated for review\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 0\n",
            "Output Files generated for review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install t5[gcp]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHUmqpuZyufK",
        "outputId": "367f2f62-623e-43fd-f688-209904b41693"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: t5[gcp] in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (1.4.0)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (2.12.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (0.6.2)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (2.2.4)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (0.5.0)\n",
            "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (0.1.21)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (1.5.3)\n",
            "Requirement already satisfied: rouge-score>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (2.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (1.10.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (0.1.99)\n",
            "Requirement already satisfied: seqio-nightly in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (0.0.15.dev20230604)\n",
            "Requirement already satisfied: six>=1.14 in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (1.16.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (4.9.2.dev202306050047)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (4.29.2)\n",
            "Requirement already satisfied: gevent in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (22.10.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (2.84.0)\n",
            "Requirement already satisfied: google-compute-engine in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (2.8.13)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (2.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from t5[gcp]) (4.1.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5[gcp]) (0.18.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5[gcp]) (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=2.7.0->t5[gcp]) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.7.0->t5[gcp]) (0.15.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.7.0->t5[gcp]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.7.0->t5[gcp]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.7.0->t5[gcp]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=2.7.0->t5[gcp]) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.7.0->t5[gcp]) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.7.0->t5[gcp]) (4.65.0)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.10/dist-packages (from gevent->t5[gcp]) (4.6)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from gevent->t5[gcp]) (6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from gevent->t5[gcp]) (67.7.2)\n",
            "Requirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent->t5[gcp]) (2.0.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->t5[gcp]) (0.21.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->t5[gcp]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->t5[gcp]) (0.1.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->t5[gcp]) (2.11.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->t5[gcp]) (4.1.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->t5[gcp]) (2.3.2)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->t5[gcp]) (2.5.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.10/dist-packages (from google-compute-engine->t5[gcp]) (2.49.0)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.10/dist-packages (from google-compute-engine->t5[gcp]) (1.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->t5[gcp]) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->t5[gcp]) (1.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->t5[gcp]) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->t5[gcp]) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->t5[gcp]) (4.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->t5[gcp]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->t5[gcp]) (2022.7.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->t5[gcp]) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->t5[gcp]) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->t5[gcp]) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->t5[gcp]) (4.9.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->t5[gcp]) (3.1.0)\n",
            "Requirement already satisfied: clu in /usr/local/lib/python3.10/dist-packages (from seqio-nightly->t5[gcp]) (0.0.9)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from seqio-nightly->t5[gcp]) (0.4.10)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from seqio-nightly->t5[gcp]) (0.4.10+cuda11.cudnn86)\n",
            "Requirement already satisfied: pyglove in /usr/local/lib/python3.10/dist-packages (from seqio-nightly->t5[gcp]) (0.3.0)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from seqio-nightly->t5[gcp]) (2.12.1)\n",
            "Requirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.10/dist-packages (from seqio-nightly->t5[gcp]) (3.20.3)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->t5[gcp]) (0.2.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->t5[gcp]) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->t5[gcp]) (1.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->t5[gcp]) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->t5[gcp]) (5.9.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->t5[gcp]) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->t5[gcp]) (2.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->t5[gcp]) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->t5[gcp]) (1.14.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tfds-nightly->t5[gcp]) (5.12.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tfds-nightly->t5[gcp]) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tfds-nightly->t5[gcp]) (3.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client->t5[gcp]) (1.59.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client->t5[gcp]) (5.3.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->t5[gcp]) (1.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->t5[gcp]) (3.0.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=2.7.0->t5[gcp]) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=2.7.0->t5[gcp]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=2.7.0->t5[gcp]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=2.7.0->t5[gcp]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=2.7.0->t5[gcp]) (3.4)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (from clu->seqio-nightly->t5[gcp]) (0.6.9)\n",
            "Requirement already satisfied: ml-collections in /usr/local/lib/python3.10/dist-packages (from clu->seqio-nightly->t5[gcp]) (0.1.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax->seqio-nightly->t5[gcp]) (0.1.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->seqio-nightly->t5[gcp]) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->seqio-nightly->t5[gcp]) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->seqio-nightly->t5[gcp]) (2.12.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (3.8.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (16.0.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (0.32.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax->clu->seqio-nightly->t5[gcp]) (1.0.5)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax->clu->seqio-nightly->t5[gcp]) (0.1.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax->clu->seqio-nightly->t5[gcp]) (0.2.1)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax->clu->seqio-nightly->t5[gcp]) (0.1.36)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax->clu->seqio-nightly->t5[gcp]) (13.3.4)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections->clu->seqio-nightly->t5[gcp]) (0.6.0.post1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (0.40.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax->clu->seqio-nightly->t5[gcp]) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax->clu->seqio-nightly->t5[gcp]) (2.14.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (2.3.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax->clu->seqio-nightly->t5[gcp]) (0.1.7)\n",
            "Requirement already satisfied: cached_property in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->clu->seqio-nightly->t5[gcp]) (1.5.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->clu->seqio-nightly->t5[gcp]) (1.5.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax->clu->seqio-nightly->t5[gcp]) (0.12.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax->clu->seqio-nightly->t5[gcp]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (2.1.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->seqio-nightly->t5[gcp]) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from t5 import evaluation\n",
        "#from transformers import t5"
      ],
      "metadata": {
        "id": "zz8cKoHIV5hr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluation.metrics.bleu(actuals, predictions, None)"
      ],
      "metadata": {
        "id": "DZu6yWD7a87m"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bleu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euZK_qfjVTWk",
        "outputId": "04bff4e6-6b90-4c5b-d4f7-211f7f27606f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 38.2206246551265}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bleu.get('bleu'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtHqKHVgb4SU",
        "outputId": "f0d785c8-f22b-4a49-ee7d-8ee12e86ba67"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38.2206246551265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "S_YmvqJ4cHwD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"model_e\" + str(configTRAIN_EPOCHS) + \"_b\" + str(round(bleu.get('bleu'), 2)) + \".pkl\""
      ],
      "metadata": {
        "id": "mdHrzdvJbYqD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3s3XS12bjDT",
        "outputId": "affb3886-c022-4376-a288-0d191255e88d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_e5_b38.22.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "xoumbbQ2cZD0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "WQrt92x9caQO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5BaZoQjgxweb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval().cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF8OXK1K6-6G",
        "outputId": "a67bbcf7-a800-4555-b4cd-8f1115ae49ca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanEdit(sentence):\n",
        "  input_ids2 = tokenizer.encode(sentence, return_tensors='pt').to(device)\n",
        "  output = model.generate(input_ids2, \n",
        "                        max_length=150, \n",
        "                        num_beams=2, \n",
        "                        repetition_penalty=2.5, \n",
        "                        length_penalty=1.0, \n",
        "                        early_stopping=True,\n",
        "                        temperature = 0.9).to(device)\n",
        "  decoded = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "  print(decoded)"
      ],
      "metadata": {
        "id": "vTSNtc7364zi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: I've been fuckin' hoes and poppin' pillies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRRkny0k7Kc7",
        "outputId": "db25d343-4ba0-4f13-9d23-64cd4be020b9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean: I've been fuckin' hoes and poppin' pillies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: I'm like, 'Fuck you and fuck her, too'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-6ZxTnU-bVY",
        "outputId": "e59c3cd4-b78e-4e84-efee-188691db7d9d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": I'm like, 'Fuck you and love her too'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: She eat my dick like it's free, free\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAShOAYZ_UPC",
        "outputId": "212f21d3-01f7-4da3-a0bd-dbc326ab26ac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean: She eat my stuff like it's free, free\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: If you sexy than flaunt it\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyfdw7jB_W6v",
        "outputId": "7a65d27d-c151-4382-c282-6cdde1e76a3b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean: If you're sexy than flaunt it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eBJuMrdII-Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Don't stop, pop that pussy!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wro7d7sVGPX7",
        "outputId": "5e5aadd8-f34d-4d15-a84e-846f89d6efd2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Don't stop, pop that girl!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Takin' all the liquor straight, never chase that (never)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TOoI9X-GPhT",
        "outputId": "abe4cb5d-9f68-4ca9-b0ab-27aca79702d9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Takin' all the liquor straight, never chase that (never)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Booty moving weight like she on the block (Woo!)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCt-3ovLGdjP",
        "outputId": "8ba2d71d-be4a-4289-c117-fa9dbb8875ff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Move like she on the block (Woo!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Take a bullet straight through my brain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43aUUnPHGPo1",
        "outputId": "7f7e19fd-59c3-4cbc-cfe1-4e7d932a09b8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": Take a look straight through my brain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: High off of love, drunk from her hate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0otPfc0EGPvI",
        "outputId": "40c547e9-c236-442d-a2c3-83c34f1ba70a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean: High off of love, drunk from her love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: I just fucked two bitches 'fore I saw you\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E2aNDqSGwnK",
        "outputId": "7f3d1576-24a3-40fc-e169-91d02772910a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I just fucked two girls 'fore I saw you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: My pussy feel like a lake\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxf9gdVyG1sg",
        "outputId": "eb6ca9dd-9b43-4a18-ad92-08c2cd921f48"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": My little feel like a lake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: If you wanna see some real ass, baby, here's your chance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwhS9jMkG7di",
        "outputId": "6772b5c2-efc9-4ff0-a8cf-945163ecf7de"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": If you wanna see some real girls, baby, here's your chance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Bitch better have my money\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP9emiwcHDXm",
        "outputId": "14580c16-19b8-46ff-8805-555c66fd7ab9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bitch better have my money\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Someone watchin' this shit close, yep, close\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM-ZucCyHZIg",
        "outputId": "2137a352-f416-47f0-ee00-af6414b5d04b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean: Someone watchin' this stuff close, yeah, close\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Turn a dyke bitch out have her fuckin' boys\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYD7ExBvHgCU",
        "outputId": "5f76fcac-08b1-4dcf-d498-c3a59b4c6d04"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": Turn a dyke bitch out have her own boys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Bought a lil' ice yeah, I wanna beat that dope like Ike, yeah\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRPuTAisHw-g",
        "outputId": "e04b120f-ec98-4353-b911-010bb7729154"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I wanna beat that dope like Ike, yeah\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Hit the strip club, we be letting bands go\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQA1YPAsH3be",
        "outputId": "591691aa-c6ce-4730-cfdf-9211657c5b38"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit the strip club, we be letting bands go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanEdit(\"Clean: Smoke in the air, binge drinkin'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opgp6tb2H8tu",
        "outputId": "232bf595-9e14-4ec6-b8f0-e31ec8dd333a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean: Smoke in the air, binge drinking\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}